---
layout: default
title: Pangool - Introduction
name: introduction_2
---
<div class="hero-unit">
	<h2>Introduction to Pangool: Topical Word Count (2/3)</h2>
	</p>
	<p>This introduction guides you into the basics of Pangool through a Word-Count-like example.<p>
	<p>Now, you'll see how to leverage Pangool <strong>instance-based configuration for managing trivial state information</strong>.</p>
</div>

<h2>Managing state</h2>

<p>
We’ll modify the previous example slightly in order to show another of Pangool's key feature: 
augmented Hadoop API by accepting instances instead of static classes.
</p>

<p>
A common use case when dealing with textual content is stop-word filtering. Let's modify the previous example slightly for being able to filter according to a stop word list. See the code below:
</p>

<p>
You can check the full code of this example on github <a href='https://github.com/datasalt/pangool/blob/master/examples/src/main/java/com/datasalt/pangool/examples/topicalwordcount/TopicalWordCountWithStopWords.java'>by clicking here</a>.
</p>

<pre class="prettyprint" id="java">
 public static class StopWordMapper extends TokenizeMapper {

   private Set<String> stopWords = new HashSet<String>();
		
   public StopWordMapper(List<String> stopWords) {
     this.stopWords.addAll(stopWords);
     this.stopWords = Collections.unmodifiableSet(this.stopWords);
   }

   protected void emitTuple(Collector collector) throws IOException, InterruptedException {
     if(stopWords.contains(tuple.get("word"))) {
       return;
     }
     super.emitTuple(collector);
   }
 }
</pre>

<p>
As you can see, this Mapper extends the one we created <a href='introduction.html'>in the first part of the introduction</a> 
and adds a List of stop words by constructor.
</p>

<p> 
It then uses this list to filter words before writing the tuples to the intermediate output. 
</p>

<p>
Let’s see how we can use this Mapper when creating a Pangool Job:
</p>

<pre class="prettyprint" id="java">
 List<String> stopWords = Files.readLines(new File(args[2]), Charset.forName("UTF-8"));
	
 TupleMRBuilder cg = new TupleMRBuilder(conf, "Pangool Topical Word Count With Stop Words");
 cg.addIntermediateSchema(TopicalWordCount.getSchema());
 cg.setGroupByFields("topic", "word");
 StopWordMapper mapper = new StopWordMapper(stopWords);
 cg.addInput(new Path(args[0]), new HadoopInputFormat(TextInputFormat.class), mapper);
</pre>

<p>
That’s it! Pangool will serialize the instance and recover it when needed. 
Remember that all state in your classes must be Serializable. If some of your class fields are not <i>Serializable</i>, 
remember to instantiate them in the <code>setup()</code> method instead of instantiating them directly in the class definition. 
</p>

<p>
You can run this example by doing:
</p>

<pre class="prettyprint" id="java">
 hadoop jar $PANGOOL_EXAMPLES_JAR topical_word_count_with_stop_words [input] [output]
</pre>

<p>
You can also use an input data generator for generating random input for this example:
</p>

<pre class="prettyprint" id="java">
 hadoop jar $PANGOOL_EXAMPLES_JAR topical_word_count_gen_data [out-file] [nRegisters] [nTopics]
</pre>

<p><a class="btn btn-primary btn-large" href="introduction_3.html">What's next? Secondary sort & Named outputs!</a></p>