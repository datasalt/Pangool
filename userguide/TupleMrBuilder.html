---
layout: user_guide
title: Pangool - User guide - Creating Pangool Jobs
---
<div class="hero-unit">
	<h1>Pangool User Guide</h1>
</div>

<h1>Creating Pangool Jobs</h1>

<p>
The class <code>TupleMRBuilder</code> is the one responsible for building Pangool jobs. Here
you have an example extracted from the 
<a href="https://github.com/datasalt/pangool/blob/master/examples/src/main/java/com/datasalt/pangool/examples/urlresolution/UrlResolution.java">
URL resolution example</a>:
</p>

<pre class="prettyprint" id="java">
 TupleMRBuilder mr = new TupleMRBuilder(conf, "Pangool Url Resolution");
 mr.addIntermediateSchema(getURLMapSchema());
 mr.addIntermediateSchema(getURLRegisterSchema());
 mr.addInput(new Path(input1), new TupleTextInputFormat(getURLMapSchema(), false, false, '\t', 
 	NO_QUOTE_CHARACTER, NO_ESCAPE_CHARACTER, null, null), new IdentityTupleMapper());
 mr.addInput(new Path(input2), new TupleTextInputFormat(getURLRegisterSchema(), false, false, '\t', 
 	NO_QUOTE_CHARACTER, NO_ESCAPE_CHARACTER, null, null), new IdentityTupleMapper());
 mr.setFieldAliases("urlMap", new Aliases().add("url", "nonCanonicalUrl"));
 mr.setGroupByFields("url");
 mr.setOrderBy(new OrderBy().add("url", Order.ASC).addSchemaOrder(Order.ASC));
 mr.setSpecificOrderBy("urlRegister", new OrderBy().add("timestamp", Order.ASC));
 mr.setTupleReducer(new Handler());
 mr.setOutput(new Path(output), new TupleTextOutputFormat(getURLRegisterSchema(), false, '\t', 
 	NO_QUOTE_CHARACTER, NO_ESCAPE_CHARACTER), ITuple.class, NullWritable.class);
</pre>

<div class="alert alert-info">
<p>
<strong>Important:</strong> The method cleanUpInstanceFiles() needs to be called to properly remove all serialized objects that Pangool uses for each Job. 
</p>
</div>

<p>
Let's analyze the code line by line. The following line creates the <code>TupleMRBuilder</code>.
The parameters are a Hadoop <code>Configuration</code> and the name of the generated job:
</p>

<pre class="prettyprint" id="java">
 TupleMRBuilder mr = new TupleMRBuilder(conf, "Pangool Url Resolution");
</pre>

<h3>Intermediate Schemas</h3>

<p>
We need to define which schemas (if more than one) will be allowed for <code>TupleMapper</code> output (intermediate schemas).
</p>

<pre class="prettyprint" id="java">
 mr.addIntermediateSchema(getURLMapSchema());
 mr.addIntermediateSchema(getURLRegisterSchema());
</pre>

<div class="alert alert-info">
<p>
<strong>Important:</strong> The order in which intermediate schemas are provided to the 
<code>TupleMRBuilder</code> is important. It may have impact in the
default order in which tuples of different schemas will be received in the 
<code>TupleReducer</code>. See <a href="joins.html">Reduce-side joins</a> for 
more information.
</p>
</div>

<h3>Grouping and Sorting</h3>

<p>
An important configuration parameter is how tuples must be grouped
and sorted before reaching the <code>TupleReducer</code>. For this particular case, 
tuples are grouped by <code>url</code>. Sorting is not specified. 
</p> 

<pre class="prettyprint" id="java">
 mr.setGroupByFields("url");
</pre>

<p>
See <a href="group_and_sort.html">Grouping and Sorting</a> for more possibilities.
</p>

<h3>Setting the job input and associated TupleMapper</h3>

<p>
You can configure a particular <code>TupleMapper</code> per each input path. 
That is, you can have several inputs, and each of them can be processed differently.
For this particular example, we are configuring two input paths (input1 and input2),
 an both are processed by a default Pangool's mapper IdentityTupleMapper, because we don't need
 to perform any specific mapping logic.</p>
 
<p>
For parsing input text files into Tuples we use "TupleTextInputFormat" (see <a href='text_io.html'>Text Input/Output</a> for more information).</p>

<pre class="prettyprint" id="java">
mr.addInput(new Path(input1), new TupleTextInputFormat(getURLMapSchema(), false, false, '\t', 
	NO_QUOTE_CHARACTER, NO_ESCAPE_CHARACTER, null, null), new IdentityTupleMapper());
mr.addInput(new Path(input2), new TupleTextInputFormat(getURLRegisterSchema(), false, false, '\t', 
	NO_QUOTE_CHARACTER, NO_ESCAPE_CHARACTER, null, null), new IdentityTupleMapper());
</pre>

<div class="alert alert-info">
<p>
Pangool can also use old Hadoop-native InputFormats like TextInputFormat. For that, a wrapper class <code>HadoopInputFormat</code> exists which receives a class as argument (unlike in Hadoop, in Pangool everything are instances: Mapper, Reducer, Input/Output Format, ...)     
</p>
</div>

<h4 class="small">Using (binary) Tuples as input/output</h4>

<p>
Pangool can serialize Tuples in an efficient binary format with the method <code>setTupleOutput()</code>.
</p>

<p>
You can then configure your job to read binary tuples by using the method <code>addTupleInput()</code> instead of <code>addInput()</code>.
</p>

<p>You can also use <code>addTupleInput(Path path, Schema targetSchema, ...)</code> to read binary tuple files whose schema don't necessarily match another target schema.
Fields that are not used anymore will be skipped, and new fields not present in the input file will be received as null values.</p>

<h3>Setting the reducer</h3>

<p>
The reducer is configured with the method <code>setTupleReducer()</code>. It should 
extend the class <code>TupleReducer</code>:
</p>

<pre class="prettyprint" id="java">
 mr.setTupleReducer(new Handler());
</pre>

<p>
A combiner class can be provided by using the method <code>setTupleCombiner()</code>. You 
can see an example of its use in the  
<a href="https://github.com/datasalt/pangool/blob/master/examples/src/main/java/com/datasalt/pangool/examples/topicalwordcount/TopicalWordCount.java">
topical word count</a> example. 
</p>

<h3>Setting the job output</h3>

<p>
The next step is to set the job output. We want to store the output into
the folder <code>output</code> using Pangool's Text I/O:
</p>

<pre class="prettyprint" id="java">
 mr.setOutput(new Path(output), new TupleTextOutputFormat(getURLRegisterSchema(), false, '\t', 
 	NO_QUOTE_CHARACTER, NO_ESCAPE_CHARACTER), ITuple.class, NullWritable.class);
</pre>

<p>
Note that <code>TupleMapper</code> and <code>TupleReducer</code> can also have more than one output by using
<a href="named_outputs.html">named outputs</a>.
</p>

<h3>And finally... build and launch your job!</h3>

<p>
The following code builds a Hadoop job, executes it, and block until completion. 
That's all, folks!
</p>

<pre class="prettyprint" id="java">
 mr.createJob().waitForCompletion(true);
</pre>

<p><a class="btn btn-primary btn-large" href="group_and_sort.html">Next: Group by / Sort by &raquo;</a></p>